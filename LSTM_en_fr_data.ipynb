{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "984fa795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang bắt đầu tải và giải nén dữ liệu...\n",
      "--> File train.en đã tồn tại, bỏ qua.\n",
      "--> File train.fr đã tồn tại, bỏ qua.\n",
      "--> File val.en đã tồn tại, bỏ qua.\n",
      "--> File val.fr đã tồn tại, bỏ qua.\n",
      "--> File test.en đã tồn tại, bỏ qua.\n",
      "--> File test.fr đã tồn tại, bỏ qua.\n",
      "\n",
      "Danh sách file trong thư mục data/ (Kiểm tra xem dung lượng có > 0KB không):\n",
      "- test.en: 60.62 KB\n",
      "- test.fr: 70.57 KB\n",
      "- train.en: 1759.02 KB\n",
      "- train.fr: 2061.82 KB\n",
      "- val.en: 61.81 KB\n",
      "- val.fr: 72.19 KB\n",
      "\n",
      "--- Kiểm tra nội dung file train.en (5 dòng đầu) ---\n",
      "Two young, White males are outside near many bushes.\n",
      "Several men in hard hats are operating a giant pulley system.\n",
      "A little girl climbing into a wooden playhouse.\n",
      "A man in a blue shirt is standing on a ladder cleaning a window.\n",
      "Two men are at the stove preparing food.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# 1. Cấu hình thư mục\n",
    "data_dir = 'NLP_PROJECT/data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    print(f\"Đã tạo thư mục: {data_dir}\")\n",
    "\n",
    "# 2. Danh sách link file NÉN (.gz) chính thức hiện tại\n",
    "# Lưu ý: Các file này đều có đuôi .gz\n",
    "urls = {\n",
    "    \"train.en\": \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/train.en.gz\",\n",
    "    \"train.fr\": \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/train.fr.gz\",\n",
    "    \"val.en\": \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/val.en.gz\",\n",
    "    \"val.fr\": \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/val.fr.gz\",\n",
    "    \"test.en\": \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2016_flickr.en.gz\",\n",
    "    \"test.fr\": \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2016_flickr.fr.gz\"\n",
    "}\n",
    "\n",
    "print(\"Đang bắt đầu tải và giải nén dữ liệu...\")\n",
    "\n",
    "for filename, url in urls.items():\n",
    "    # Đường dẫn file nén (.gz) và file đích (txt)\n",
    "    gz_path = os.path.join(data_dir, filename + \".gz\") # ví dụ: train.en.gz\n",
    "    final_path = os.path.join(data_dir, filename)      # ví dụ: train.en\n",
    "    \n",
    "    # Kiểm tra nếu file đích chưa có thì mới tải\n",
    "    if not os.path.exists(final_path):\n",
    "        print(f\"--> Đang tải: {filename}...\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Tải file .gz về\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                with open(gz_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                \n",
    "                # 2. Giải nén file .gz thành file text\n",
    "                with gzip.open(gz_path, 'rb') as f_in:\n",
    "                    with open(final_path, 'wb') as f_out:\n",
    "                        shutil.copyfileobj(f_in, f_out)\n",
    "                \n",
    "                # 3. Xóa file .gz cho nhẹ máy\n",
    "                os.remove(gz_path)\n",
    "                print(f\"    Đã tải và giải nén xong: {filename}\")\n",
    "            else:\n",
    "                print(f\"    LỖI: Link hỏng hoặc file không tồn tại (Status {response.status_code})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    Lỗi ngoại lệ khi xử lý {filename}: {e}\")\n",
    "    else:\n",
    "        print(f\"--> File {filename} đã tồn tại, bỏ qua.\")\n",
    "\n",
    "# 3. Kiểm tra kết quả\n",
    "print(\"\\nDanh sách file trong thư mục data/ (Kiểm tra xem dung lượng có > 0KB không):\")\n",
    "files = os.listdir(data_dir)\n",
    "for f in files:\n",
    "    size = os.path.getsize(os.path.join(data_dir, f))\n",
    "    print(f\"- {f}: {size/1024:.2f} KB\")\n",
    "\n",
    "# Test thử nội dung file train.en xem có phải tiếng Anh không\n",
    "print(\"\\n--- Kiểm tra nội dung file train.en (5 dòng đầu) ---\")\n",
    "try:\n",
    "    with open(os.path.join(data_dir, 'train.en'), 'r', encoding='utf-8') as f:\n",
    "        for i in range(5):\n",
    "            print(f.readline().strip())\n",
    "except Exception as e:\n",
    "    print(f\"Chưa đọc được file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b34241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 5 dòng đầu tiên của file train.en ---\n",
      "Two young, White males are outside near many bushes.\n",
      "Several men in hard hats are operating a giant pulley system.\n",
      "A little girl climbing into a wooden playhouse.\n",
      "A man in a blue shirt is standing on a ladder cleaning a window.\n",
      "Two men are at the stove preparing food.\n",
      "\n",
      "=> ĐÂY LÀ FILE TEXT, KHÔNG PHẢI RAR. BẠN CÓ THỂ DÙNG TRỰC TIẾP!\n"
     ]
    }
   ],
   "source": [
    "# Chạy thử đoạn này để xem nội dung file\n",
    "path = 'NLP_PROJECT/data/train.en' # Đảm bảo đường dẫn đúng với thư mục bạn lưu\n",
    "\n",
    "try:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        print(\"--- 5 dòng đầu tiên của file train.en ---\")\n",
    "        for i in range(5):\n",
    "            print(f.readline().strip())\n",
    "    print(\"\\n=> ĐÂY LÀ FILE TEXT, KHÔNG PHẢI RAR. BẠN CÓ THỂ DÙNG TRỰC TIẾP!\")\n",
    "except Exception as e:\n",
    "    print(f\"Có lỗi: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "837beb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.9.1\n",
      "Uninstalling torch-2.9.1:\n",
      "  Successfully uninstalled torch-2.9.1\n",
      "Found existing installation: torchtext 0.18.0\n",
      "Uninstalling torchtext-0.18.0:\n",
      "  Successfully uninstalled torchtext-0.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping torchvision as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.8.11)\n",
      "Collecting torch\n",
      "  Using cached torch-2.9.1-cp310-cp310-win_amd64.whl.metadata (30 kB)\n",
      "Collecting torchtext\n",
      "  Using cached torchtext-0.18.0-cp310-cp310-win_amd64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (5.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Using cached torch-2.9.1-cp310-cp310-win_amd64.whl (111.0 MB)\n",
      "Using cached torchtext-0.18.0-cp310-cp310-win_amd64.whl (1.9 MB)\n",
      "Installing collected packages: torch, torchtext\n",
      "\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   -------------------- ------------------- 1/2 [torchtext]\n",
      "   -------------------- ------------------- 1/2 [torchtext]\n",
      "   -------------------- ------------------- 1/2 [torchtext]\n",
      "   ---------------------------------------- 2/2 [torchtext]\n",
      "\n",
      "Successfully installed torch-2.9.1 torchtext-0.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.8/12.8 MB 5.6 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.4/12.8 MB 10.3 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 10.0 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.8/12.8 MB 10.5 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.2/12.8 MB 10.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 12.3/12.8 MB 10.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 10.4 MB/s  0:00:01\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.8.0--- ĐÃ CÀI ĐẶT XONG ---\n",
      "Vui lòng nhấn nút 'Restart' (mũi tên vòng tròn) trên thanh công cụ của Notebook để áp dụng thay đổi!\n",
      "\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "     ---------------------------------------- 0.0/16.3 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/16.3 MB 5.6 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 2.4/16.3 MB 11.2 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 5.2/16.3 MB 10.3 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 6.8/16.3 MB 11.0 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 10.5/16.3 MB 11.3 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 12.8/16.3 MB 11.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 15.2/16.3 MB 11.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 16.3/16.3 MB 10.9 MB/s  0:00:01\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# 1. Gỡ cài đặt các bản đang bị xung đột\n",
    "!pip uninstall -y torch torchtext torchvision\n",
    "\n",
    "!pip install spacy torch torchtext\n",
    "\n",
    "# 3. Tải mô hình ngôn ngữ cho Spacy (nếu chưa có)\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download fr_core_news_sm\n",
    "\n",
    "print(\"--- ĐÃ CÀI ĐẶT XONG ---\")\n",
    "print(\"Vui lòng nhấn nút 'Restart' (mũi tên vòng tròn) trên thanh công cụ của Notebook để áp dụng thay đổi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ac08329",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Could not load this library: C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torchtext\\lib\\libtorchtext.pyd",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\torch\\_ops.py:1488\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1488\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\ctypes\\__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] The specified procedure could not be found",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequence\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vocab\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\torchtext\\__init__.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m     _WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     20\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\torchtext\\_extension.py:64\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\torchtext\\_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\torchtext\\_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[1;34m(lib)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\torch\\_ops.py:1490\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m   1488\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mCDLL(path)\n\u001b[0;32m   1489\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1490\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load this library: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1491\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "\u001b[1;31mOSError\u001b[0m: Could not load this library: C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torchtext\\lib\\libtorchtext.pyd"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "import spacy\n",
    "import io\n",
    "\n",
    "# --- 1. CÀI ĐẶT TOKENIZER (SPACY) ---\n",
    "# Tải model ngôn ngữ (nếu chưa tải ở bước trước thì bỏ comment dòng dưới)\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download fr_core_news_sm\n",
    "\n",
    "print(\"Đang tải Spacy models...\")\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "spacy_fr = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"Tách từ tiếng Anh và đảo ngược (để tăng hiệu suất LSTM nếu muốn - tùy chọn)\"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize_fr(text):\n",
    "    \"\"\"Tách từ tiếng Pháp\"\"\"\n",
    "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "# --- 2. XÂY DỰNG TỪ ĐIỂN (VOCABULARY) ---\n",
    "def build_vocab(filepath, tokenizer):\n",
    "    counter = Counter()\n",
    "    with io.open(filepath, encoding=\"utf8\") as f:\n",
    "        for string_ in f:\n",
    "            counter.update(tokenizer(string_.lower().strip()))\n",
    "    \n",
    "    # Tạo vocab object\n",
    "    # min_freq=2: Từ nào xuất hiện dưới 2 lần sẽ coi là <unk> để giảm nhiễu\n",
    "    v = vocab(counter, min_freq=2, specials=['<unk>', '<pad>', '<sos>', '<eos>'])\n",
    "    \n",
    "    # Gán index mặc định cho từ lạ (unknown)\n",
    "    v.set_default_index(v['<unk>']) \n",
    "    return v\n",
    "\n",
    "print(\"Đang xây dựng từ điển (Vocabulary)...\")\n",
    "en_vocab = build_vocab('NLP_PROJECT/data/train.en', tokenize_en)\n",
    "fr_vocab = build_vocab('NLP_PROJECT/data/train.fr', tokenize_fr)\n",
    "\n",
    "print(f\"- Số lượng từ vựng Tiếng Anh: {len(en_vocab)}\")\n",
    "print(f\"- Số lượng từ vựng Tiếng Pháp: {len(fr_vocab)}\")\n",
    "\n",
    "# Lưu các index đặc biệt để dùng sau này\n",
    "PAD_IDX = en_vocab['<pad>']\n",
    "SOS_IDX = en_vocab['<sos>']\n",
    "EOS_IDX = en_vocab['<eos>']\n",
    "\n",
    "# --- 3. CUSTOM DATASET CLASS ---\n",
    "class Multi30kDataset(Dataset):\n",
    "    def __init__(self, src_path, trg_path, src_vocab, trg_vocab, src_tokenizer, trg_tokenizer):\n",
    "        self.src_data = open(src_path, encoding='utf-8').read().split('\\n')\n",
    "        self.trg_data = open(trg_path, encoding='utf-8').read().split('\\n')\n",
    "        \n",
    "        # Đồng bộ số lượng dòng (bỏ dòng rỗng cuối file)\n",
    "        min_len = min(len(self.src_data), len(self.trg_data))\n",
    "        self.src_data = self.src_data[:min_len]\n",
    "        self.trg_data = self.trg_data[:min_len]\n",
    "        \n",
    "        # Lọc bỏ các cặp câu rỗng\n",
    "        self.src_data, self.trg_data = zip(*[(s, t) for s, t in zip(self.src_data, self.trg_data) if s.strip() and t.strip()])\n",
    "\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.src_data[idx].lower().strip()\n",
    "        trg_text = self.trg_data[idx].lower().strip()\n",
    "\n",
    "        # Tokenize -> List of strings -> List of Indices\n",
    "        src_indices = [self.src_vocab[token] for token in self.src_tokenizer(src_text)]\n",
    "        trg_indices = [self.trg_vocab[token] for token in self.trg_tokenizer(trg_text)]\n",
    "\n",
    "        # Thêm <sos> vào đầu và <eos> vào cuối\n",
    "        src_indices = [SOS_IDX] + src_indices + [EOS_IDX]\n",
    "        trg_indices = [SOS_IDX] + trg_indices + [EOS_IDX]\n",
    "\n",
    "        return torch.tensor(src_indices), torch.tensor(trg_indices)\n",
    "\n",
    "# --- 4. COLLATE FUNCTION (PADDING) ---\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = [], []\n",
    "    for src_item, trg_item in batch:\n",
    "        src_batch.append(src_item)\n",
    "        trg_batch.append(trg_item)\n",
    "    \n",
    "    # Pad các câu về cùng độ dài lớn nhất trong batch\n",
    "    # padding_value=PAD_IDX: Điền vào chỗ trống bằng index của <pad>\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=False) \n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    \n",
    "    return src_batch, trg_batch\n",
    "\n",
    "# --- 5. KHỞI TẠO DATALOADER ---\n",
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Đang tạo DataLoaders (Batch Size: {BATCH_SIZE})...\")\n",
    "\n",
    "train_dataset = Multi30kDataset(\n",
    "    'NLP_PROJECT/data/train.en', 'NLP_PROJECT/data/train.fr', \n",
    "    en_vocab, fr_vocab, tokenize_en, tokenize_fr\n",
    ")\n",
    "val_dataset = Multi30kDataset(\n",
    "    'NLP_PROJECT/data/val.en', 'NLP_PROJECT/data/val.fr', \n",
    "    en_vocab, fr_vocab, tokenize_en, tokenize_fr\n",
    ")\n",
    "test_dataset = Multi30kDataset(\n",
    "    'NLP_PROJECT/data/test.en', 'NLP_PROJECT/data/test.fr', \n",
    "    en_vocab, fr_vocab, tokenize_en, tokenize_fr\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# --- 6. KIỂM TRA THỬ 1 BATCH ---\n",
    "src, trg = next(iter(train_loader))\n",
    "print(\"\\n--- Kiểm tra cấu trúc Batch ---\")\n",
    "print(f\"Shape Source (seq_len, batch_size): {src.shape}\")\n",
    "print(f\"Shape Target (seq_len, batch_size): {trg.shape}\")\n",
    "print(\"Done! Dữ liệu đã sẵn sàng để train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4515b3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
