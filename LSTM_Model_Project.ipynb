{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FIX LỖI MÔI TRƯỜNG KAGGLE ---\n",
    "# Lệnh này sẽ chạy trên server Kaggle để gỡ thư viện gây xung đột\n",
    "import os\n",
    "\n",
    "# Gỡ bỏ Cupy (thủ phạm chính gây xung đột với Spacy/Scipy hiện tại)\n",
    "os.system(\"pip uninstall -y cupy cupy-cuda11x cupy-cuda12x\")\n",
    "\n",
    "# Cài đặt lại Numpy và Scipy bản ổn định để tránh lỗi \"ufunc\"\n",
    "os.system(\"pip install 'numpy<2.0' 'scipy<1.13'\")\n",
    "\n",
    "# --- KẾT THÚC FIX LỖI ---\n",
    "print(\"Đã sửa xong môi trường! Bắt đầu chạy code chính...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thông tin thiết bị (GPU/CPU) và phiên bản Torch\n",
    "import torch, platform\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Thiết bị đang dùng: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Số GPU khả dụng: {torch.cuda.device_count()}\")\n",
    "    print(f\"Tên GPU[0]: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Python: {platform.python_version()} | Platform: {platform.platform()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T10:01:07.739372Z",
     "iopub.status.busy": "2025-12-02T10:01:07.738453Z",
     "iopub.status.idle": "2025-12-02T10:01:09.059944Z",
     "shell.execute_reply": "2025-12-02T10:01:09.058953Z",
     "shell.execute_reply.started": "2025-12-02T10:01:07.739337Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# 1. Cấu hình thư mục\n",
    "data_dir = 'data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    print(f\"Đã tạo thư mục: {data_dir}\")\n",
    "\n",
    "# 2. Danh sách link file NÉN (.gz) chính thức hiện tại\n",
    "# Lưu ý: Các file này đều có đuôi .gz\n",
    "urls = {\n",
    "    \"train.en\": \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/train.en.gz\",\n",
    "    \"train.fr\": \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/train.fr.gz\",\n",
    "    \"val.en\": \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/val.en.gz\",\n",
    "    \"val.fr\": \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/val.fr.gz\",\n",
    "    \"test.en\": \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2016_flickr.en.gz\",\n",
    "    \"test.fr\": \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2016_flickr.fr.gz\"\n",
    "}\n",
    "\n",
    "print(\"Đang bắt đầu tải và giải nén dữ liệu...\")\n",
    "\n",
    "for filename, url in urls.items():\n",
    "    # Đường dẫn file nén (.gz) và file đích (txt)\n",
    "    gz_path = os.path.join(data_dir, filename + \".gz\") # ví dụ: train.en.gz\n",
    "    final_path = os.path.join(data_dir, filename)      # ví dụ: train.en\n",
    "    \n",
    "    # Kiểm tra nếu file đích chưa có thì mới tải\n",
    "    if not os.path.exists(final_path):\n",
    "        print(f\"--> Đang tải: {filename}...\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Tải file .gz về\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                with open(gz_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                \n",
    "                # 2. Giải nén file .gz thành file text\n",
    "                with gzip.open(gz_path, 'rb') as f_in:\n",
    "                    with open(final_path, 'wb') as f_out:\n",
    "                        shutil.copyfileobj(f_in, f_out)\n",
    "                \n",
    "                # 3. Xóa file .gz cho nhẹ máy\n",
    "                os.remove(gz_path)\n",
    "                print(f\"    Đã tải và giải nén xong: {filename}\")\n",
    "            else:\n",
    "                print(f\"    LỖI: Link hỏng hoặc file không tồn tại (Status {response.status_code})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    Lỗi ngoại lệ khi xử lý {filename}: {e}\")\n",
    "    else:\n",
    "        print(f\"--> File {filename} đã tồn tại, bỏ qua.\")\n",
    "\n",
    "# 3. Kiểm tra kết quả\n",
    "print(\"\\nDanh sách file trong thư mục data/ (Kiểm tra xem dung lượng có > 0KB không):\")\n",
    "files = os.listdir(data_dir)\n",
    "for f in files:\n",
    "    size = os.path.getsize(os.path.join(data_dir, f))\n",
    "    print(f\"- {f}: {size/1024:.2f} KB\")\n",
    "\n",
    "# Test thử nội dung file train.en xem có phải tiếng Anh không\n",
    "print(\"\\n--- Kiểm tra nội dung file train.en (5 dòng đầu) ---\")\n",
    "try:\n",
    "    with open(os.path.join(data_dir, 'train.en'), 'r', encoding='utf-8') as f:\n",
    "        for i in range(5):\n",
    "            print(f.readline().strip())\n",
    "except Exception as e:\n",
    "    print(f\"Chưa đọc được file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi động môi trường PyTorch an toàn (tắt Dynamo/ONNX trước khi import torch)\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_DYNAMO\"] = \"0\"\n",
    "os.environ[\"TORCH_ONNX_DISABLE\"] = \"1\"\n",
    "print(\"Đã set PYTORCH_ENABLE_DYNAMO=0 và TORCH_ONNX_DISABLE=1. Hãy Restart Kernel rồi chạy lại notebook từ đầu.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T10:04:04.838432Z",
     "iopub.status.busy": "2025-12-02T10:04:04.837926Z",
     "iopub.status.idle": "2025-12-02T10:04:10.311012Z",
     "shell.execute_reply": "2025-12-02T10:04:10.309463Z",
     "shell.execute_reply.started": "2025-12-02T10:04:04.838403Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import io\n",
    "\n",
    "# --- Simple Vocabulary (no torchtext) ---\n",
    "class Vocabulary:\n",
    "    def __init__(self, counter, min_freq=2, specials=('\\u003Cunk\\u003E','\\u003Cpad\\u003E','\\u003Csos\\u003E','\\u003Ceos\\u003E')):\n",
    "        self.specials = list(specials)\n",
    "        self.itos = self.specials.copy()\n",
    "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
    "        for token, freq in counter.items():\n",
    "            if freq >= min_freq and token not in self.stoi:\n",
    "                self.stoi[token] = len(self.itos)\n",
    "                self.itos.append(token)\n",
    "        self.default_index = self.stoi['\\u003Cunk\\u003E']\n",
    "    def __getitem__(self, token):\n",
    "        return self.stoi.get(token, self.default_index)\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "# --- 1. TOKENIZER (SPACY) ---\n",
    "print(\"Đang tải Spacy models...\")\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "spacy_fr = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize_fr(text):\n",
    "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "# --- 2. BUILD VOCAB ---\n",
    "def build_vocab(filepath, tokenizer):\n",
    "    counter = Counter()\n",
    "    with io.open(filepath, encoding=\"utf8\") as f:\n",
    "        for string_ in f:\n",
    "            counter.update(tokenizer(string_.lower().strip()))\n",
    "    return Vocabulary(counter, min_freq=2)\n",
    "\n",
    "print(\"Đang xây dựng từ điển (Vocabulary)...\")\n",
    "en_vocab = build_vocab('data/train.en', tokenize_en)\n",
    "fr_vocab = build_vocab('data/train.fr', tokenize_fr)\n",
    "\n",
    "print(f\"- Số lượng từ vựng Tiếng Anh: {len(en_vocab)}\")\n",
    "print(f\"- Số lượng từ vựng Tiếng Pháp: {len(fr_vocab)}\")\n",
    "\n",
    "PAD_IDX = en_vocab['\\u003Cpad\\u003E']\n",
    "SOS_IDX = en_vocab['\\u003Csos\\u003E']\n",
    "EOS_IDX = en_vocab['\\u003Ceos\\u003E']\n",
    "\n",
    "# --- 3. DATASET ---\n",
    "class Multi30kDataset(Dataset):\n",
    "    def __init__(self, src_path, trg_path, src_vocab, trg_vocab, src_tokenizer, trg_tokenizer):\n",
    "        self.src_data = open(src_path, encoding='utf-8').read().split('\\n')\n",
    "        self.trg_data = open(trg_path, encoding='utf-8').read().split('\\n')\n",
    "        min_len = min(len(self.src_data), len(self.trg_data))\n",
    "        self.src_data = self.src_data[:min_len]\n",
    "        self.trg_data = self.trg_data[:min_len]\n",
    "        self.src_data, self.trg_data = zip(*[(s, t) for s, t in zip(self.src_data, self.trg_data) if s.strip() and t.strip()])\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.src_data[idx].lower().strip()\n",
    "        trg_text = self.trg_data[idx].lower().strip()\n",
    "        src_indices = [self.src_vocab[token] for token in self.src_tokenizer(src_text)]\n",
    "        trg_indices = [self.trg_vocab[token] for token in self.trg_tokenizer(trg_text)]\n",
    "        src_indices = [SOS_IDX] + src_indices + [EOS_IDX]\n",
    "        trg_indices = [SOS_IDX] + trg_indices + [EOS_IDX]\n",
    "        return torch.tensor(src_indices), torch.tensor(trg_indices)\n",
    "\n",
    "# --- 4. COLLATE ---\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = [], []\n",
    "    for src_item, trg_item in batch:\n",
    "        src_batch.append(src_item)\n",
    "        trg_batch.append(trg_item)\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    return src_batch, trg_batch\n",
    "\n",
    "# --- 5. DATALOADER ---\n",
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Đang tạo DataLoaders (Batch Size: {BATCH_SIZE})...\")\n",
    "train_dataset = Multi30kDataset('data/train.en', 'data/train.fr', en_vocab, fr_vocab, tokenize_en, tokenize_fr)\n",
    "val_dataset = Multi30kDataset('data/val.en', 'data/val.fr', en_vocab, fr_vocab, tokenize_en, tokenize_fr)\n",
    "test_dataset = Multi30kDataset('data/test.en', 'data/test.fr', en_vocab, fr_vocab, tokenize_en, tokenize_fr)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# --- 6. SAMPLE BATCH ---\n",
    "src, trg = next(iter(train_loader))\n",
    "print(\"\\n--- Kiểm tra cấu trúc Batch ---\")\n",
    "print(f\"Shape Source (seq_len, batch_size): {src.shape}\")\n",
    "print(f\"Shape Target (seq_len, batch_size): {trg.shape}\")\n",
    "print(\"Done! Dữ liệu đã sẵn sàng để train.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xây dựng mô hình LSTM Seq2Seq\n",
    "\n",
    "Phần này triển khai Encoder–Decoder LSTM với các tham số có thể chỉnh sửa.\n",
    "\n",
    "- Bạn có thể sửa các tham số trong mục \"Config\" ở đầu cell tiếp theo (hidden size, embedding dim, số layer, dropout, tỉ lệ teacher forcing, batch size, số epoch, learning rate, v.v.).\n",
    "- Loop huấn luyện in ra train/val loss theo epoch và lưu `best_model.pt` khi val loss giảm.\n",
    "- Hàm `translate(sentence)` ở cuối để suy luận nhanh trên câu tiếng Anh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config: chỉnh các tham số tuỳ ý\n",
    "CONFIG = {\n",
    "    \"vocab_en\": None,   # sẽ set từ cell preprocessing\n",
    "    \"vocab_fr\": None,   # sẽ set từ cell preprocessing\n",
    "    \"pad_idx\": None,    # sẽ set từ cell preprocessing\n",
    "    \"sos_idx\": None,    # sẽ set từ cell preprocessing\n",
    "    \"eos_idx\": None,    # sẽ set từ cell preprocessing\n",
    "    \"embedding_dim\": 512,   # 256–512\n",
    "    \"hidden_size\": 1024,     \n",
    "    \"num_layers\": 8,\n",
    "    \"dropout\": 0.3,         # 0.3–0.5\n",
    "    \"teacher_forcing\": 0.5,\n",
    "    \"lr\": 1e-35,\n",
    "    \"epochs\": 10,           # 10–20\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "}\n",
    "\n",
    "# Lấy các đối tượng/từ cell preprocess\n",
    "CONFIG[\"vocab_en\"] = en_vocab\n",
    "CONFIG[\"vocab_fr\"] = fr_vocab\n",
    "CONFIG[\"pad_idx\"] = PAD_IDX\n",
    "CONFIG[\"sos_idx\"] = SOS_IDX\n",
    "CONFIG[\"eos_idx\"] = EOS_IDX\n",
    "\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_size, num_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=False)\n",
    "    def forward(self, src):  # src: [seq_len, batch]\n",
    "        embedded = self.embedding(src)  # [seq_len, batch, emb_dim]\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        # outputs: [seq_len, batch, hidden]; hidden/cell: [num_layers, batch, hidden]\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_size, num_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=False)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    def forward(self, input_token, hidden, cell):  # input_token: [batch]\n",
    "        input_token = input_token.unsqueeze(0)     # [1, batch]\n",
    "        embedded = self.embedding(input_token)     # [1, batch, emb_dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        logits = self.fc(output.squeeze(0))        # [batch, vocab_size]\n",
    "        return logits, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, sos_idx, eos_idx, pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.sos_idx = sos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.pad_idx = pad_idx\n",
    "        self.device = device\n",
    "    def forward(self, src, trg, teacher_forcing=0.5):\n",
    "        # src: [src_len, batch], trg: [trg_len, batch]\n",
    "        batch = src.size(1)\n",
    "        trg_len = trg.size(0)\n",
    "        vocab_size = self.decoder.fc.out_features\n",
    "        outputs = torch.zeros(trg_len, batch, vocab_size, device=self.device)\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input_token = torch.full((batch,), self.sos_idx, dtype=torch.long, device=self.device)\n",
    "        for t in range(trg_len):\n",
    "            logits, hidden, cell = self.decoder(input_token, hidden, cell)\n",
    "            outputs[t] = logits\n",
    "            teacher = random.random() < teacher_forcing\n",
    "            input_token = trg[t] if teacher else torch.argmax(logits, dim=1)\n",
    "        return outputs\n",
    "\n",
    "# Khởi tạo model\n",
    "enc = Encoder(\n",
    "    vocab_size=len(CONFIG[\"vocab_en\"]),\n",
    "    emb_dim=CONFIG[\"embedding_dim\"],\n",
    "    hidden_size=CONFIG[\"hidden_size\"],\n",
    "    num_layers=CONFIG[\"num_layers\"],\n",
    "    dropout=CONFIG[\"dropout\"],\n",
    "    pad_idx=CONFIG[\"pad_idx\"],\n",
    ")\n",
    "\n",
    "dec = Decoder(\n",
    "    vocab_size=len(CONFIG[\"vocab_fr\"]),\n",
    "    emb_dim=CONFIG[\"embedding_dim\"],\n",
    "    hidden_size=CONFIG[\"hidden_size\"],\n",
    "    num_layers=CONFIG[\"num_layers\"],\n",
    "    dropout=CONFIG[\"dropout\"],\n",
    "    pad_idx=CONFIG[\"pad_idx\"],\n",
    ")\n",
    "\n",
    "model = Seq2Seq(enc, dec, CONFIG[\"sos_idx\"], CONFIG[\"eos_idx\"], CONFIG[\"pad_idx\"], CONFIG[\"device\"]).to(CONFIG[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khắc phục lỗi Torch Dynamo/ONNX import trên Anaconda\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_DYNAMO\"] = \"0\"  # Tắt TorchDynamo trước khi import torch\n",
    "# os.environ[\"TORCH_ONNX_DISABLE\"] = \"1\"  # Tuỳ chọn: tắt các phần ONNX nếu cần\n",
    "\n",
    "import torch\n",
    "print(\"Đã tắt TorchDynamo qua biến môi trường. Tiếp tục cấu hình mô hình...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nếu dùng CUDA 12.x\n",
    "!pip install --upgrade cupy-cuda12x\n",
    "\n",
    "# Hoặc nếu dùng CUDA 11.x\n",
    "!pip install --upgrade cupy-cuda11x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huấn luyện (có Early Stopping và Scheduler ReduceLROnPlateau)\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=CONFIG[\"pad_idx\"])  # bỏ qua pad\n",
    "optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
    "\n",
    "# Scheduler: giảm learning rate khi val_loss không cải thiện\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, min_lr=1e-6, verbose=True)\n",
    "\n",
    "best_val = float('inf')\n",
    "epochs_no_improve = 0  # Early Stopping: đếm số epoch không cải thiện\n",
    "early_stopping_patience = 3  # Dừng sớm nếu không giảm sau 3 epoch (có thể chỉnh)\n",
    "\n",
    "def run_epoch(dataloader, train=True):\n",
    "    total_loss = 0.0\n",
    "    model.train(train)\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for src_batch, trg_batch in dataloader:\n",
    "            src_batch = src_batch.to(CONFIG[\"device\"])  # [src_len, batch]\n",
    "            trg_batch = trg_batch.to(CONFIG[\"device\"])  # [trg_len, batch]\n",
    "            outputs = model(src_batch, trg_batch, teacher_forcing=CONFIG[\"teacher_forcing\"])  # [trg_len, batch, vocab]\n",
    "            # Dịch mục tiêu sang next-token (bỏ token đầu)\n",
    "            logits = outputs[:-1].reshape(-1, outputs.size(-1))\n",
    "            target = trg_batch[1:].reshape(-1)\n",
    "            loss = criterion(logits, target)\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / max(1, len(dataloader))\n",
    "\n",
    "for epoch in range(1, CONFIG[\"epochs\"] + 1):\n",
    "    train_loss = run_epoch(train_loader, train=True)\n",
    "    val_loss = run_epoch(val_loader, train=False)\n",
    "\n",
    "    # Bước scheduler dựa trên val_loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{CONFIG['epochs']} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | lr={optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    # Early Stopping & lưu best model\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\"→ Saved best_model.pt (val_loss improved)\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement for {epochs_no_improve} epoch(s)\")\n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            print(f\"Early stopping triggered (patience={early_stopping_patience}).\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suy luận (Inference)\n",
    "\n",
    "def detokenize_fr(indices, vocab_fr):\n",
    "    # Bỏ các specials và dừng ở <eos>\n",
    "    itos = vocab_fr.itos\n",
    "    tokens = []\n",
    "    for idx in indices:\n",
    "        tok = itos[idx]\n",
    "        if tok in (\"\\u003Cpad\\u003E\", \"\\u003Csos\\u003E\"):\n",
    "            continue\n",
    "        if tok == \"\\u003Ceos\\u003E\":\n",
    "            break\n",
    "        tokens.append(tok)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "@torch.no_grad()\n",
    "def translate(sentence_en: str, max_len: int = 50):\n",
    "    model.eval()\n",
    "    device = CONFIG[\"device\"]\n",
    "    src_tokens = tokenize_en(sentence_en.lower().strip())\n",
    "    src_indices = [CONFIG[\"sos_idx\"]] + [CONFIG[\"vocab_en\"][t] for t in src_tokens] + [CONFIG[\"eos_idx\"]]\n",
    "    src = torch.tensor(src_indices, dtype=torch.long, device=device).unsqueeze(1)  # [seq_len, 1]\n",
    "    hidden, cell = model.encoder(src)\n",
    "    cur = torch.tensor([CONFIG[\"sos_idx\"]], dtype=torch.long, device=device)  # [1]\n",
    "    out_indices = []\n",
    "    for _ in range(max_len):\n",
    "        logits, hidden, cell = model.decoder(cur, hidden, cell)\n",
    "        cur = torch.argmax(logits, dim=1)\n",
    "        out_indices.append(cur.item())\n",
    "        if cur.item() == CONFIG[\"eos_idx\"]:\n",
    "            break\n",
    "    return detokenize_fr(out_indices, CONFIG[\"vocab_fr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Đánh giá (Evaluation): BLEU & Perplexity\n",
    "# import math\n",
    "# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# # Tính BLEU trung bình trên tập test\n",
    "# def compute_bleu_on_test(max_samples: int = None):\n",
    "#     model.eval()\n",
    "#     smoothie = SmoothingFunction().method3\n",
    "#     total_bleu = 0.0\n",
    "#     count = 0\n",
    "#     examples = []  # lưu 5 ví dụ minh hoạ\n",
    "#     # Duyệt dữ liệu thô để lấy cặp câu gốc\n",
    "#     src_lines = open('data/test.en', encoding='utf-8').read().strip().split('\\n')\n",
    "#     trg_lines = open('data/test.fr', encoding='utf-8').read().strip().split('\\n')\n",
    "#     n = min(len(src_lines), len(trg_lines))\n",
    "#     if max_samples is not None:\n",
    "#         n = min(n, max_samples)\n",
    "#     for i in range(n):\n",
    "#         src = src_lines[i].strip()\n",
    "#         trg = trg_lines[i].strip()\n",
    "#         if not src or not trg:\n",
    "#             continue\n",
    "#         # Dịch bằng model (greedy)\n",
    "#         pred = translate(src, max_len=50)\n",
    "#         # Token hoá tham chiếu và dự đoán (sử dụng tokenizer spaCy đã có)\n",
    "#         ref_tokens = tokenize_fr(trg.lower().strip())\n",
    "#         hyp_tokens = pred.lower().strip().split()\n",
    "#         # BLEU câu\n",
    "#         bleu_i = sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=smoothie)\n",
    "#         total_bleu += bleu_i\n",
    "#         count += 1\n",
    "#         if len(examples) < 5:\n",
    "#             examples.append({\n",
    "#                 'src': src,\n",
    "#                 'ref': trg,\n",
    "#                 'hyp': pred,\n",
    "#                 'bleu': bleu_i,\n",
    "#             })\n",
    "#     avg_bleu = total_bleu / max(1, count)\n",
    "#     return avg_bleu, examples\n",
    "\n",
    "# # Perplexity từ loss (ví dụ dùng val_loss cuối cùng)\n",
    "# def loss_to_perplexity(loss_value: float):\n",
    "#     try:\n",
    "#         return math.exp(loss_value)\n",
    "#     except OverflowError:\n",
    "#         return float('inf')\n",
    "\n",
    "# # Chạy đánh giá và in kết quả\n",
    "# avg_bleu, samples = compute_bleu_on_test(max_samples=1000)  # có thể giảm số lượng để nhanh hơn\n",
    "# print(f\"BLEU trung bình (test, max_samples=1000): {avg_bleu:.4f}\")\n",
    "\n",
    "# try:\n",
    "#     # nếu biến best_val tồn tại từ cell train, tính perplexity\n",
    "#     ppl = loss_to_perplexity(best_val)\n",
    "#     print(f\"Perplexity (ước từ best val_loss): {ppl:.2f}\")\n",
    "# except NameError:\n",
    "#     print(\"Perplexity: chưa có 'best_val' (hãy chạy cell huấn luyện)\")\n",
    "\n",
    "# print(\"\\n— 5 ví dụ minh hoạ —\")\n",
    "# for i, ex in enumerate(samples, 1):\n",
    "#     print(f\"[{i}] EN: {ex['src']}\")\n",
    "#     print(f\"    REF_FR: {ex['ref']}\")\n",
    "#     print(f\"    HYP_FR: {ex['hyp']}\")\n",
    "#     print(f\"    BLEU: {ex['bleu']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo alias cho bộ test 2016 nếu thiếu (dùng test.en/test.fr)\n",
    "import os, shutil\n",
    "aliases = [\n",
    "    ('data/test.en', 'data/test_2016_flickr.en'),\n",
    "    ('data/test.fr', 'data/test_2016_flickr.fr'),\n",
    "]\n",
    "for src, dst in aliases:\n",
    "    if not os.path.exists(dst):\n",
    "        if os.path.exists(src):\n",
    "            shutil.copyfile(src, dst)\n",
    "            print(f\"Đã tạo: {dst} từ {src}\")\n",
    "        else:\n",
    "            print(f\"Thiếu nguồn: {src} — hãy chạy cell tải dữ liệu đầu tiên\")\n",
    "    else:\n",
    "        print(f\"Đã có: {dst}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tải thêm các bộ test năm 2017 và 2018 (nếu có)\n",
    "import os, requests, gzip, shutil\n",
    "data_dir = 'data'\n",
    "extra_urls = {\n",
    "    \"test_2017_flickr.en\": \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2017_flickr.en.gz\",\n",
    "    \"test_2017_flickr.fr\": \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2017_flickr.fr.gz\",\n",
    "    \"test_2018_flickr.en\": \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2018_flickr.en.gz\",\n",
    "    \"test_2018_flickr.fr\": \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2018_flickr.fr.gz\",\n",
    "}\n",
    "print(\"Đang kiểm tra và tải thêm bộ test (2017/2018) nếu cần...\")\n",
    "for filename, url in extra_urls.items():\n",
    "    gz_path = os.path.join(data_dir, filename + \".gz\")\n",
    "    final_path = os.path.join(data_dir, filename)\n",
    "    if os.path.exists(final_path):\n",
    "        print(f\"- Đã có {filename}, bỏ qua.\")\n",
    "        continue\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200:\n",
    "            with open(gz_path, 'wb') as f: f.write(r.content)\n",
    "            with gzip.open(gz_path, 'rb') as f_in, open(final_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "            os.remove(gz_path)\n",
    "            print(f\"- Tải xong: {filename}\")\n",
    "        else:\n",
    "            print(f\"- Không tải được {filename} (HTTP {r.status_code})\")\n",
    "    except Exception as e:\n",
    "        print(f\"- Lỗi khi tải {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đánh giá đa bộ test: 2016 / 2017 / 2018\n",
    "import math\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from pathlib import Path\n",
    "@torch.no_grad()\n",
    "def compute_bleu_for_files(src_path: str, trg_path: str, max_samples: int = None):\n",
    "    model.eval()\n",
    "    smoothie = SmoothingFunction().method3\n",
    "    total_bleu, count = 0.0, 0\n",
    "    examples = []\n",
    "    if not (Path(src_path).exists() and Path(trg_path).exists()):\n",
    "        print(f\"Không tìm thấy file: {src_path} hoặc {trg_path}\")\n",
    "        return None, []\n",
    "    src_lines = open(src_path, encoding='utf-8').read().strip().split('\\n')\n",
    "    trg_lines = open(trg_path, encoding='utf-8').read().strip().split('\\n')\n",
    "    n = min(len(src_lines), len(trg_lines))\n",
    "    if max_samples is not None:\n",
    "        n = min(n, max_samples)\n",
    "    for i in range(n):\n",
    "        src = src_lines[i].strip()\n",
    "        ref = trg_lines[i].strip()\n",
    "        if not src or not ref:\n",
    "            continue\n",
    "        hyp = translate(src, max_len=50)\n",
    "        try:\n",
    "            ref_tokens = tokenize_fr(ref.lower().strip())  # dùng spaCy nếu có\n",
    "        except Exception:\n",
    "            ref_tokens = ref.lower().strip().split()       # fallback đơn giản\n",
    "        hyp_tokens = hyp.lower().strip().split()\n",
    "        bleu_i = sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=smoothie)\n",
    "        total_bleu += bleu_i\n",
    "        count += 1\n",
    "        if len(examples) < 5:\n",
    "            examples.append({'src': src, 'ref': ref, 'hyp': hyp, 'bleu': bleu_i})\n",
    "    if count == 0:\n",
    "        return 0.0, examples\n",
    "    return total_bleu / count, examples\n",
    "def loss_to_perplexity(loss_value: float):\n",
    "    try:\n",
    "        return math.exp(loss_value)\n",
    "    except OverflowError:\n",
    "        return float('inf')\n",
    "# Cố gắng load best_model nếu có\n",
    "try:\n",
    "    model.load_state_dict(torch.load('best_model.pt', map_location=CONFIG['device']))\n",
    "    model.eval()\n",
    "    print('Đã load: best_model.pt')\n",
    "except Exception as e:\n",
    "    print(f\"Không load được best_model.pt (tiếp tục dùng model hiện tại): {e}\")\n",
    "# Danh sách bộ test\n",
    "test_sets = {\n",
    "    '2016_flickr': ('data/test_2016_flickr.en', 'data/test_2016_flickr.fr'),\n",
    "    '2017_flickr': ('data/test_2017_flickr.en', 'data/test_2017_flickr.fr'),\n",
    "    '2018_flickr': ('data/test_2018_flickr.en', 'data/test_2018_flickr.fr'),\n",
    "}\n",
    "results = {}\n",
    "for name, (src_p, trg_p) in test_sets.items():\n",
    "    print(f\"\\n=== Đánh giá {name} ===\")\n",
    "    avg_bleu, examples = compute_bleu_for_files(src_p, trg_p, max_samples=1000)\n",
    "    if avg_bleu is None:\n",
    "        continue\n",
    "    results[name] = avg_bleu\n",
    "    print(f\"BLEU trung bình: {avg_bleu:.4f}\")\n",
    "    try:\n",
    "        ppl = loss_to_perplexity(best_val)\n",
    "        print(f\"Perplexity (ước từ best val_loss): {ppl:.2f}\")\n",
    "    except NameError:\n",
    "        print(\"Perplexity: chưa có 'best_val' (hãy chạy cell huấn luyện)\")\n",
    "    print(\"— 3 ví dụ minh hoạ —\")\n",
    "    for i, ex in enumerate(examples[:3], 1):\n",
    "        print(f\"[{i}] EN: {ex['src']}\")\n",
    "        print(f\"    REF_FR: {ex['ref']}\")\n",
    "        print(f\"    HYP_FR: {ex['hyp']}\")\n",
    "        print(f\"    BLEU: {ex['bleu']:.4f}\\n\")\n",
    "print(\"\\nTổng hợp BLEU theo bộ test:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"- {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
